{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics in Spreadsheets\n",
    "\n",
    "#### Statistics is just a piece of information from large quantity of data. It describes data.\n",
    "\n",
    "- Mode : A number that appears most often in the dataset.\n",
    "- We can the mean using the **`=AVERAGE(B2:B11)`** function.\n",
    "- **`=MEDIAN(B2:B11)`** to calculate the median.\n",
    "- **`=MODE(B2:B11)`** to calculate mode.\n",
    "\n",
    "### Data point distance from average\n",
    "- *Variance* : Variance measures how dispersed a dataset is from its mean. The smaller the variance the less spread the data is. Conversely, large differences between data points increase the variance.\n",
    "- A : 37,37,37 | B : 10, 14, 10, 10 | C : 10, 14, 100, 10\n",
    "- Column A variance : 0, Column B : 3 , Column C : 1476.75\n",
    "- Column C has an outlier as a result its variance is maximum.\n",
    "- **`=VARP(A1:A14)`** to calculate variance.\n",
    "- Variance is the average of squared values. Thus variance is different from the original sample values making it less intutive.\n",
    "- **Most often we need to make sense of the variation by putting it in the scale of the original data.** `This is done by taking the square root of the variance, called standard deviation`\n",
    "- After taking the variance with **`VARP`** we can use **`SQRT`**, squareroot to calculate the **standard deviation**\n",
    "- More easy we can use **`STDEVP`** to get standard deviation\n",
    "\n",
    "#### Standard deviation as a unit of measure\n",
    "- Std dev scores shows how a data point relates to the distribution.\n",
    "- =average(10, 14, 10, 10) = 11 | =stdevp(10, 14, 10, 10) = 1.73 | new data point : 12.73\n",
    "- Subtracting new data point from standard dev gives back the mean | 12.73 - 1.73 = 11\n",
    "- **Thus this new data point is exactly one standard deviation away from the mean**\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images/1_std.JPG\" width=\"350\" title=\"1 std-dev\">\n",
    "</p>\n",
    "\n",
    "#### Another statistic for understanding a distribution is percentile \n",
    "- Ordering a distribution & calculating the percentage of values below a specific point will tell us its **percentile**\n",
    "- **Quartiles** are percentiles that segment the data into 4 chunks.\n",
    "- To get the popular percentiles in sheets we use the **=QUARTILE(b1:b5, 1)**\n",
    "\n",
    "- Calculating standard deviations\n",
    "There is a lot of seasonal variability in the train data. To better understand this, we will calculate the standard deviation to understand how \"spread out\" the data is from the mean average.\n",
    "\n",
    "To calculate the standard deviation:\n",
    "Calculate the variance of the data.\n",
    "Take the SQRT(), or square root, of the variance.\n",
    "\n",
    "- The QUARTILE() function accepts an array of values followed by an integer 1 to 4 to declare the specific quartile.\n",
    "Quartile 4: The maximum value in the data.\n",
    "Quartile 3: 75% of the data is less than the third quartile.\n",
    "Quartile 2: The median of the data.\n",
    "Quartile 1: The smallest values 25% of the data.\n",
    "\n",
    "- A useful statistic computed from the quartiles is the \"interquartile range\" (IQR), which lies between the Q1 and Q3 and represents the middle 50% of the data.\n",
    "\n",
    "### Standardizing Data\n",
    "- Many real world datasets have variables that are measured on different scales. For e.g, height might me measured in feet and weight might be measured in pounds. This poses a problem because variables on different scales are harder to compare, and it may lead to misinterpretation of the importance of a particular column - that column may appear more important simply because it has larger values than another, while in reality it may actually have a very similar distribution to the column with smaller values.\n",
    "- **The solution to this problem is to standardize the data so that all the variables are on the same scale**\n",
    "- In statistics, standardization centers a dataset's distribution around the mean of the data and calculates the number of std devs away from the mean each point is.\n",
    "- We can standardize our data by calculating **z-scores**, also known as standard scores.\n",
    "- **The z-score measures how far a value is from the mean using standard deviations.**\n",
    "- TO calculate the z-score of a data pt, subtract the mean and divide by the std-dev\n",
    "- **=STANDARDIZE(DATA POINT, MEAN, STD-DEV)** to calculate z-score\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images/z_scores.JPG\" width=\"350\" title=\"z scores\">\n",
    "</p>\n",
    "\n",
    "- As we can see, the new data points despite being 10 times larger, the distance of each data point to their respective sample's mean & std-dev are the same as in the first column, and this allows us to easily compare the two columns.\n",
    "\n",
    "### Visualizing Distributions\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images/common_dist.JPG\" width=\"350\" title=\"common distribution\">\n",
    "</p>\n",
    "\n",
    "- A lot of the things we measure in the world looks something like above.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images/common_dist2.JPG\" width=\"350\" title=\"common distribution\">\n",
    "</p>\n",
    "\n",
    "- Above distribution is of pizza delivery times.\n",
    "- In both cases, the highest point is in the middle of the distribution, or the most frequent value is the highest point.\n",
    "- The resulting shape is similar to a bell so this distribution is often called a **bell curve**. It is technically called the **normal distribution**.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images/hist.JPG\" width=\"350\" title=\"normal histogram\">\n",
    "</p>\n",
    "\n",
    "- Conceptually, a normal distribution is symmetrical and shaped like a bell.\n",
    "- In normal distributions ,the mean, median and modes have similar values. As a result, not only is the distribution centered at the mean, median and mode but the frequencies of these values decrease symmetrically so other summary stats are affected like quartiles and std -devs.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images/dists.JPG\" width=\"350\" title=\"distributions\">\n",
    "</p>\n",
    "\n",
    "- There are many distribution shapes, some of the common ones are shown above. For each, the summary statistics will change. For e.g the mean is affected by outliers, or extreme values in the data, however the median is not.\n",
    "- So in a `skewed distribution` these values will differ **unlike the normal or symmetrical distribution where mean and median are very close.**\n",
    "- Often in stats we want to test how \"normal\" the data is. Even though we can see a bell curve shape, there are two mathematical aspects to a normal distribution. \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./images/skew.JPG\" width=\"350\" title=\"normal distributions\">\n",
    "</p>\n",
    "\n",
    "- First, how much does the data lean or skew from the most frequent value. In the visual its shown with the red arrows. A normal distribution will have little to now skew, represented by the vertical red line.\n",
    "- Second, to measure symmetry and how the tails trail off, we review the blue lines.\n",
    "- These aspects, leaning and trailing off are measured in **skew** and **kurtosis**.\n",
    "- In sheets we use **=skew(a1:a10)** and **kurt(a1:a10)** , to get leaning and trailing off statistics. \n",
    "- There are many opinions on acceptable values for skew and kurtosis to describe a normal distribution. Typically, values between **-2 & 2 for both skew and kurtosis** can indicate a **normal distribution**\n",
    "- To add histogram in sheets : Go to insert -> then chart -> select histogram from chart type drop down and finally declare the data range.\n",
    "\n",
    "#### Is the data \"normally\" distributed?\n",
    "- Earlier, we noted that the mean and median savings values were almost equal. The savings histogram was also roughly symmetrical and resembled a bell curve.\n",
    "- Now we want to use statistics to verify if the distribution is approximately symmetrical or \"normally distributed\". To do so, we will calculate the skew and kurtosis.\n",
    "- **Kurtosis**: Calculated using `KURT()`. Measures how the tails behave. It identifies how values are concentrated around the mean and how they trail away from it in the tails.\n",
    "- **Skew**: Calculated using `SKEW()`. Measures how symmetrical the distribution is. 0 means the distribution is exactly symmetrical. Values above or below 0 indicate that there are more values above or below the mean.\n",
    "\n",
    "### Visualizing Correlations\n",
    "- We will transition from histogram which visualizes a single variables to a scatter plot which explores relationship among two variables.\n",
    "- In  sheets we can add a imaginary line passing through our scattered data points. Its called a trend line to help see the overall trend relationship.\n",
    "- To add a trendline in sheet : click \"insert\" then \"chart\" then select \"scatter chart\" from the drop down , then click customize in the dialog and under \"series\" check the \"trendline\" box.\n",
    "- The trend line is added or fit so that the distance between the line and each point is minimized.\n",
    "\n",
    "#### Stats about the trendline\n",
    "- In sheets we can get the slope and intercept of the trend line using the **=LINEST(rangeX, rangeY)** or line estimate function. It accepts two ranges and will return the slope then the intercept.\n",
    "- The slope can be interesting because it declares the trend relationship between the variables. For e.g a slope of 1.5 means that as the X varaible increases 1 then the Y varaible will increase by 1.5. IF the slope is -1, then if the slope variable decreases by 1 then so does the Y variable.\n",
    "- correlation can be calculated using **CORREL()**, ranges between -1 and 1. \n",
    "- 0 correlation means there is no relationship between variables.\n",
    "- Positive correlation indicate that as one variable increases, the other also increases.\n",
    "- Negative correlation values signify that as one variable increases, the other decreases.\n",
    "- Spreadsheets have 3 formulas for calculating the y-intercept and slope given two variables.\n",
    "**SLOPE()** - will return the slope of a trend line or linear regression representing the linear change in one unit to another.\n",
    "**INTERCEPT()** - returns the value where the trendline will intersect the y-axis.\n",
    "**LINEST()** - calculates both the slope & the intercept of two variables using the least-squares method.\n",
    "\n",
    "### Bar Charts\n",
    "- Visualize non-numeric data.\n",
    "- **`COUNTIF()`** function can been to used to count the number of \"1\"s and \"0\"s. \n",
    "\n",
    "## Statistical Hypothesis Testing\n",
    "\n",
    "### Central to stats - Sampling\n",
    "- **Population** : In stats, a population is defined as an **entire** distribution of similar observations or events.\n",
    "- Often its costly and time consuming to work with entire population, so its better to **sample** the population. A sample is a subset of the population's observations.\n",
    "- **Central Limit Theorem** : It finds that if we repeatedly randomly sample independently from any distribution, skewed or not, **the resulting sample will be normal**. Using an appropriate sample size along with the central limit theorem helps overcome the problem of using data from non-normal populations.\n",
    "- The more data that's gathered in a sample, the more certainty exists in the resulting statistics. The normalcy of the sample, proven in the central limit theorem let's us make statistical inferences from the sample to the population which leads us to hypothesis testing.\n",
    "- **Central limit theorem** is defined as `If a sample size from an independent random variable is *large enough* then the sampling distribution will be normal or nearly normal`. This lets us make inferences to the population.\n",
    "- However, \"large enough\" is vague. The size is dictated by two factors. First, exactly how precise do we need to be ? Second factor is how the population distribution behaves. The more normal the underlying population the less sample data points are needed to make accurate inferences based on the sample. If the underlying population is normal they would say a sample of 30-40 would be sufficent to make inferences about the population. \n",
    "\n",
    "#### Sampling in Spreadsheets \n",
    "- There are many ways to sample data. Sampling choices affect the statistics and as a result, what we learn about the data population. A popular method for sampling is to do it randomly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
